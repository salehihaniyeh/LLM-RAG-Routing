{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNd4FbJP/1LW4nJ4JFQcJUh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salehihaniyeh/LLM-RAG-Routing/blob/main/RAG_Routing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0Qs79Mhx2-7"
      },
      "outputs": [],
      "source": [
        "! pip install langchain_community tiktoken langchain-groq langchainhub chromadb langchain youtube-transcript-api pytube --use-deprecated=legacy-resolver\n",
        "!pip install langchain_groq[embeddings]\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"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\"\n",
        "os.environ['GROQ_API_KEY'] = \"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\""
      ],
      "metadata": {
        "id": "9X7HBxp61m63"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logical & Semantic Routing"
      ],
      "metadata": {
        "id": "h-ykB_E21srO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Data model\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"python_docs\", \"js_docs\", \"literature_docs\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
        "    )\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0)\n",
        "structured_llm = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
        "\n",
        "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define router\n",
        "router = prompt | structured_llm"
      ],
      "metadata": {
        "id": "jMFpRbPn1o2W"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = \"\"\"Which paper answers the question:\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
        "prompt.invoke(\"french\")\n",
        "\"\"\"\n",
        "\n",
        "result1 = router.invoke({\"question\": question1})\n",
        "print(result1)\n",
        "print(result1.datasource)"
      ],
      "metadata": {
        "id": "qArLOCxF3Ria",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e0e113b-5e0d-4480-a3dd-f19d41ba2e8c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasource='literature_docs'\n",
            "literature_docs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question2 = \"\"\"Why is my code giving me errors:\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
        "prompt.invoke(\"french\")\n",
        "\"\"\"\n",
        "\n",
        "result2 = router.invoke({\"question\": question2})\n",
        "print(result2)\n",
        "print(result2.datasource)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJnlhPBgky-9",
        "outputId": "986a741c-3501-4ef8-8ca2-3cfa814c9753"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasource='python_docs'\n",
            "python_docs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_route(result):\n",
        "    if \"python_docs\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for python_docs\"\n",
        "    elif \"js_docs\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for js_docs\"\n",
        "    else:\n",
        "        ### Logic here\n",
        "        return \"literature_docs\"\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = router | RunnableLambda(choose_route)"
      ],
      "metadata": {
        "id": "9KVFgWAw32k7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"question\": question1})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TjUglP3I36CD",
        "outputId": "dd55834c-9815-4424-c2a3-8683b7b4524e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'literature_docs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Semantic Routing"
      ],
      "metadata": {
        "id": "f7yHuFvv4Gen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utils.math import cosine_similarity\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Two prompts\n",
        "ML_template = \"\"\"You are a an expert in Machine learning. \\\n",
        "You are great at answering questions about large language models (LLM) in a concise and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "medicine_template = \"\"\"You are a very good physician. You are great at answering medical questions. \\\n",
        "You are so good because you have years of experience and know human body very well and have a broad \\\n",
        "knowledge about illnesses and their symptoms.\\\n",
        "When you don't know the answer to a question you admit that you don't know.\n",
        "\n",
        "\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "# Embed prompts\n",
        "embeddings = SentenceTransformerEmbeddings()\n",
        "prompt_templates = [ML_template, medicine_template]\n",
        "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
        "\n",
        "# Route question to prompt\n",
        "def prompt_router(input):\n",
        "    # Embed question\n",
        "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
        "    # Compute similarity\n",
        "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
        "    most_similar = prompt_templates[similarity.argmax()]\n",
        "    # Chosen prompt\n",
        "    print(\"Using Machine Learning\" if most_similar == ML_template else \"Using Medicine\")\n",
        "    return PromptTemplate.from_template(most_similar)\n",
        "\n",
        "\n",
        "chain = (\n",
        "    {\"query\": RunnablePassthrough()}\n",
        "    | RunnableLambda(prompt_router)\n",
        "    | ChatGroq()\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain.invoke(\"How does RAG improve the ability of language models?\"))\n",
        "print(chain.invoke(\"I've been experiencing persistent fatigue and headaches,\\\n",
        " along with unexplained weight loss and increased thirst and urination. What is the underlying cause.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5EnFpTr4O0U",
        "outputId": "b9cba95c-038f-4b20-ec9a-1663ac2695ae"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Machine Learning\n",
            "RAG, or Retrieval-Augmented Generation, is a technique that combines a language model with a retrieval system to improve its ability to generate accurate and relevant responses.\n",
            "\n",
            "In a RAG model, the language model is enhanced with the ability to retrieve relevant information from a large corpus of documents, such as a database or the entire Wikipedia, before generating a response. This allows the model to access a wider range of information beyond its pre-trained knowledge, which can lead to more accurate and up-to-date responses.\n",
            "\n",
            "RAG can improve the ability of language models in several ways:\n",
            "\n",
            "1. Increased Factual Accuracy: By retrieving relevant information from a large corpus of documents, RAG models can generate more factually accurate responses.\n",
            "2. Improved Relevance: RAG models can generate responses that are more relevant to the user's question or input, as they have access to a wider range of information.\n",
            "3. Reduced Hallucinations: RAG models are less likely to generate hallucinated or made-up information, as they have the ability to retrieve and verify information from a reliable source.\n",
            "4. Better Generalization: RAG models can generalize better to new domains or topics, as they can retrieve and learn from relevant information on the fly.\n",
            "\n",
            "In summary, RAG improves the ability of language models by enabling them to retrieve and utilize relevant information from a large corpus of documents, leading to more accurate, relevant, and reliable responses.\n",
            "Using Medicine\n",
            "I am an artificial intelligence and while I can provide some information based on the symptoms you have described, I would highly recommend that you consult with a healthcare professional for an accurate diagnosis. The symptoms you have mentioned (persistent fatigue, headaches, unexplained weight loss, increased thirst, and increased urination) could be associated with several medical conditions, including diabetes, thyroid disorders, or certain infections. However, it is important to undergo a thorough medical evaluation to determine the underlying cause.\n"
          ]
        }
      ]
    }
  ]
}